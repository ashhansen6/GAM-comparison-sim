---
title: "Appendix"
author: "Ashton Hansen"
date: "12/9/2020"
output: html_document
---

# Packages used:
```{r}
library(tidyverse)
library(glmnet)
library(gam)
```

# Data Creation
```{r}
# Packages used:
library(tidyverse)

# Seed
set.seed(1456)

# Predictor Variables
x1 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x2 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x3 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x4 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x5 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x6 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x7 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x8 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x9 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
x10 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))

# Predicted Variables
y1 <- x1 + x2 + x6 + x7 - x5 - x8
hist(y1)
plot(y1)

y2 <- x1^2 + 0.5*x5^2 + 2*x6^2 - 1.5*x9^2 - 2*x10^2
hist(y2)
plot(y2)

y3 <- (0.5*x1^3 - 0.25*x1^2 + .2*x1 - 1.5*x2^3 + 1.2*x2^2 - 1.5*x2 + x7^3 - x7^2 + 0.35*x7 + 15)
hist(y3)
plot(y3)

# Data Frame
df <- data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, y1, y2, y3)
df <- df %>% mutate(id = row_number())

round(as.matrix(cor(df[1:10])), 4)

round(cor(df[1:10]), 4)


# Test and Training Sets
train <- sample_n(df, 800)
test <- anti_join(df, train, by = 'id')
```

# Ridge Models
## Linear Predictors
$y_1 = x_1 + x_3 - x_5 + x_6 - x_8 + x_{10}$
```{r}
set.seed(1456)
ridgecv_y1 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), nfold = 5, alpha=0)

plot(ridgecv_y1)

ridge_y1 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), lambda = ridgecv_y1$lambda.1se, alpha=0)

coefficients(ridge_y1)

ridge_y1_pred <- predict(ridge_y1, s = ridgecv_y1$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))

ridge_y1_error <- mean((test$y1 - ridge_y1_pred)^2)
ridge_y1_error
```

## Quadratic Predictors
$y_2 = x_1^2 + 0.5x_5^2 + 2x_6^2 - 1.5x_9^2 - 2x_{10}^2$
```{r}
set.seed(1456)
ridgecv_y2 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), nfold = 5, alpha=0)

plot(ridgecv_y2)

ridge_y2 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), lambda = ridgecv_y2$lambda.1se, alpha=0)

coefficients(ridge_y2)

ridge_y2_pred <- predict(ridge_y2, s = ridgecv_y2$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))

ridge_y2_error <- mean((test$y2 - ridge_y2_pred)^2)
ridge_y2_error
```

## Cubic Predictors
$y_3 = (0.5x_1^3 - 0.25x_1^2 + 0.2x_1) + (-1.5x_2^3 + 1.2x_2^2 - 1.5x_2) + (x_7^3 - x_7^2 + 0.35x_7) + 15$
```{r}
set.seed(1456)
ridgecv_y3 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), nfold = 5, alpha=0)

plot(ridgecv_y3)

ridge_y3 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), lambda = ridgecv_y3$lambda.1se, alpha=0)

coefficients(ridge_y3)

ridge_y3_pred <- predict(ridge_y3, s = ridgecv_y3$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))

ridge_y3_error <- mean((test$y3 - ridge_y3_pred)^2)
ridge_y3_error
```

# LASSO Models
## Linear Predictors
$y_1 = x_1 + x_3 - x_5 + x_6 - x_8 + x_{10}$
```{r}
set.seed(1456)
lassocv_y1 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), nfold = 5, alpha=1)

plot(lassocv_y1)

lasso_y1 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), lambda = lassocv_y1$lambda.1se, alpha=1)

coefficients(lasso_y1)

lasso_y1_pred <- predict(lasso_y1, s = lassocv_y1$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))

lasso_y1_error <- mean((test$y1 - lasso_y1_pred)^2)
lasso_y1_error
```

## Quadratic Predictors
$y_2 = x_1^2 + 0.5x_5^2 + 2x_6^2 - 1.5x_9^2 - 2x_{10}^2$
```{r}
set.seed(1456)
lassocv_y2 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), nfold = 5, alpha=1)

plot(lassocv_y2)

lasso_y2 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), lambda = lassocv_y2$lambda.1se, alpha=1)

coefficients(lasso_y2)

lasso_y2_pred <- predict(lasso_y2, s = lassocv_y2$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))

lasso_y2_error <- mean((test$y2 - lasso_y2_pred)^2)
lasso_y2_error
```

## Cubic Predictors
$y_3 = (0.5x_1^3 - 0.25x_1^2 + 0.2x_1) + (-1.5x_2^3 + 1.2x_2^2 - 1.5x_2) + (x_7^3 - x_7^2 + 0.35x_7) + 15$
```{r}
set.seed(1456)
lassocv_y3 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), nfold = 5, alpha=1)

plot(lassocv_y3)

lasso_y3 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), lambda = lassocv_y3$lambda.1se, alpha=1)

coefficients(lasso_y3)

lasso_y3_pred <- predict(lasso_y3, s = lassocv_y3$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))

lasso_y3_error <- mean((test$y3 - lasso_y3_pred)^2)
lasso_y3_error
```

# GAMs
## Linear Predictors
$y_1 = x_1 + x_3 - x_5 + x_6 - x_8 + x_{10}$
```{r}
set.seed(1458)
# Fully Linear (GAM used for Y1)
gam_y1.1 <- gam(y1 ~ x1 + x2 - x5 + x6 + x7 - x8, data = train)
summary(gam_y1.1)
gam_pred_y1.1 <- predict(gam_y1.1, test)
gam_error_y1.1 <- mean((gam_pred_y1.1 - test$y1)^2)
gam_error_y1.1

# Testing Natural spline on x1
gam_y1.2 <- gam(y1 ~ ns(x1) + x2 - x5 + x6 + x7 - x8, data = train)
summary(gam_y1.2)
gam_pred_y1.2<- predict(gam_y1.2, test)
gam_error_y1.2 <- mean((gam_pred_y1.2 - test$y1)^2)
gam_error_y1.2
```

## Quadratic Predictors
$y_2 = x_1^2 + 0.5x_5^2 + 2x_6^2 - 1.5x_9^2 - 2x_{10}^2$
```{r}
set.seed(1458)
# GAM used for Y2
gam_y2 <- gam(y2 ~ s(x1) + poly(x6, 2) + s(x9) + poly(x10, 2), data = train)
summary(gam_y2)
gam_pred_y2 <- predict(gam_y2, test)
gam_error_y2 <- mean((test$y2 - gam_pred_y2)^2)
gam_error_y2
```

## Cubic Predictors
$y_3 = (0.5x_1^3 - 0.25x_1^2 + 0.2x_1) + (-1.5x_2^3 + 1.2x_2^2 - 1.5x_2) + (x_7^3 - x_7^2 + 0.35x_7) + 15$
```{r}
set.seed(1458)
# GAM used for Y3
gam_y3 <- gam(y3 ~ poly(x1, 2) + poly(x2, 2) + poly(x7, 2), data = train)
summary(gam_y3)
gam_pred_y3 <- predict(gam_y3, test)
gam_error_y3 <- mean((test$y3 - gam_pred_y3)^2)
gam_error_y3
```

# Simulation

```{r}
# Initializations
n <- 1000 # (One Thousand Repetitions)
errors_ridge_y1 <- rep(-1, n) # Using -1 as placeholder, can check if worked
errors_ridge_y2 <- rep(-1, n)
errors_ridge_y3 <- rep(-1, n)

errors_lasso_y1 <- rep(-1, n)
errors_lasso_y2 <- rep(-1, n)
errors_lasso_y3 <- rep(-1, n)

errors_gam_y1 <- rep(-1, n)
errors_gam_y2 <- rep(-1, n)
errors_gam_y3 <- rep(-1, n)

errors <- data.frame(errors_ridge_y1, errors_ridge_y2, errors_ridge_y3, errors_lasso_y1, errors_lasso_y2, errors_lasso_y3, errors_gam_y1, errors_gam_y2, errors_gam_y3)

# Progress Bar (Ignore)
prog <- winProgressBar(title = "Progress", min = 0, max = n, width = 300)

for (r in 1:n){
  
  
  ###############
  # Making Data #
  ###############
  
  # Predictor Variables
  x1 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x2 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x3 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x4 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x5 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x6 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x7 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x8 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x9 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  x10 <- rnorm(1000, rnorm(1, 0, 5), rnorm(1, 2, 0.25))
  
  # Predicted Variables
  y1 <- x1 + x2 + x6 + x7 - x5 - x8
  y2 <- x1^2 + 0.5*x5^2 + 2*x6^2 - 1.5*x9^2 - 2*x10^2
  y3 <- (0.5*x1^3 - 0.25*x1^2 + .2*x1 - 1.5*x2^3 + 1.2*x2^2 - 1.5*x2 + x7^3 - x7^2 + 0.35*x7 + 15)
  
  # Data Frame
  df <- data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, y1, y2, y3)
  df <- df %>% mutate(id = row_number())
  
  # Test and Training Sets
  train <- sample_n(df, 800)
  test <- anti_join(df, train, by = 'id')
  
  ###########################
  # Ridge Regression Models #
  ###########################
  ridgecv_y1 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), nfold = 5, alpha=0)
  ridge_y1 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), lambda = ridgecv_y1$lambda.1se, alpha=0)
  ridge_y1_pred <- predict(ridge_y1, s = ridgecv_y1$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))
  errors$errors_ridge_y1[r] <- mean((test$y1 - ridge_y1_pred)^2)
  
  ridgecv_y2 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), nfold = 5, alpha=0)
  ridge_y2 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), lambda = ridgecv_y2$lambda.1se, alpha=0)
  ridge_y2_pred <- predict(ridge_y2, s = ridgecv_y2$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))
  errors$errors_ridge_y2[r] <- mean((test$y2 - ridge_y2_pred)^2)
  
  ridgecv_y3 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), nfold = 5, alpha=0)
  ridge_y3 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), lambda = ridgecv_y3$lambda.1se, alpha=0)
  ridge_y3_pred <- predict(ridge_y3, s = ridgecv_y3$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))
  errors$errors_ridge_y3[r] <- mean((test$y3 - ridge_y3_pred)^2)
  
  ################
  # LASSO Models #
  ################
  lassocv_y1 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), nfold = 5, alpha=1)
  lasso_y1 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y1))), lambda = lassocv_y1$lambda.1se, alpha=1)
  lasso_y1_pred <- predict(lasso_y1, s = lassocv_y1$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))
  errors$errors_lasso_y1[r] <- mean((test$y1 - lasso_y1_pred)^2)
  
  lassocv_y2 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), nfold = 5, alpha=1)
  lasso_y2 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y2))), lambda = lassocv_y2$lambda.1se, alpha=1)
  lasso_y2_pred <- predict(lasso_y2, s = lassocv_y2$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))
  errors$errors_lasso_y2[r] <- mean((test$y2 - lasso_y2_pred)^2)
  
  lassocv_y3 <- cv.glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), nfold = 5, alpha=1)
  lasso_y3 <- glmnet(x=as.matrix(subset(train, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))), y=as.matrix(subset(train, select = c(y3))), lambda = lassocv_y3$lambda.1se, alpha=1)
  lasso_y3_pred <- predict(lasso_y3, s = lassocv_y3$lambda.1se, as.matrix(subset(test, select = c(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10))))
  errors$errors_lasso_y3[r] <- mean((test$y3 - lasso_y3_pred)^2)
  
  ########
  # GAMs #
  ########
  gam_y1 <- gam(y1 ~ x1 + x2 - x5 + x6 + x7 - x8, data = train)
  gam_pred_y1 <- predict(gam_y1, test)
  errors$errors_gam_y1[r] <- mean((gam_pred_y1 - test$y1)^2)
  
  gam_y2 <- gam(y2 ~ s(x1) + poly(x6, 2) + s(x9) + poly(x10, 2), data = train)
  gam_pred_y2 <- predict(gam_y2, test)
  errors$errors_gam_y2[r] <- mean((test$y2 - gam_pred_y2)^2)
  
  gam_y3 <- gam(y3 ~ poly(x1, 2) + poly(x2, 2) + poly(x7, 2), data = train)
  gam_pred_y3 <- predict(gam_y3, test)
  errors$errors_gam_y3[r] <- mean((test$y3 - gam_pred_y3)^2)
  
  setWinProgressBar(prog, r, title = paste(round(r/n*100, 2), "% Complete"))
}
close(prog)

colnames(errors) <- c("ridge.y1", "ridge.y2", "ridge.y3", "lasso.y1", "lasso.y2", "lasso.y3", "gam.y1", "gam.y2", "gam.y3")
summary(errors)

ggplot(errors) +
  geom_histogram(aes(x=ridge.y1), fill = "pink", col = "black") +
  ggtitle("Ridge Regression Error (Y1)")
ggplot(errors) +
  geom_histogram(aes(x=lasso.y1), fill = "lightgreen", col = "black") +
  ggtitle("LASSO Error (Y1)")
ggplot(errors) +
  geom_histogram(aes(x=gam.y1), fill = "cornflowerblue", col = "black") +
  ggtitle("GAM Error (Y1)")

ggplot(errors) +
  geom_histogram(aes(x=ridge.y2), fill = "pink", col = "black") +
  ggtitle("Ridge Regression Error (Y2)")
ggplot(errors) +
  geom_histogram(aes(x=lasso.y2), fill = "lightgreen", col = "black") +
  ggtitle("LASSO Error (Y2)")
ggplot(errors) +
  geom_histogram(aes(x=gam.y2), fill = "cornflowerblue", col = "black") +
  ggtitle("GAM Error (Y2)")

ggplot(errors) +
  geom_histogram(aes(x=ridge.y3), fill = "pink", col = "black") +
  ggtitle("Ridge Regression Error (Y3)")
ggplot(errors) +
  geom_histogram(aes(x=lasso.y3), fill = "lightgreen", col = "black") +
  ggtitle("LASSO Error (Y3)")
ggplot(errors) +
  geom_histogram(aes(x=gam.y3), fill = "cornflowerblue", col = "black") +
  ggtitle("GAM Error (Y3)")
```